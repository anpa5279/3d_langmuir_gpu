#!/bin/bash
nodes=1
cpus=128
ntasks=1
jobn=sgs_force_smag_comparison_cpus
#rm -rf outputs/$jobn
mkdir outputs/$jobn

cp langmuir_turb_sgs_cpu.jl outputs/$jobn/
cp langmuir_turb_forcing_cpu.jl outputs/$jobn/
cp stokes.jl outputs/$jobn/
cp smagorinsky_forcing.jl outputs/$jobn/

cd outputs/$jobn/
touch Project.toml

cat > launch.sh << EoF_s
#! /bin/bash
env LD_LIBRARY_PATH=\$LD_LIBRARY_PATH
exec \$*
EoF_s

chmod +x launch.sh

cat > EXEC_STEP << EXEC
#!/bin/sh

#PBS -A UCUB0166 
#PBS -N $jobn
#PBS -q main
#PBS -j oe
#PBS -l job_priority=premium
#PBS -l walltime=1:00:00
#PBS -l select=$nodes:ncpus=$cpus:mpiprocs=$cpus

export JULIA_NUM_PRECOMPILE_TASKS=$cpus
export JULIA_NUM_THREADS=$cpus

module --force purge
module load ncarenv/23.09 gcc/13.2.0 cray-mpich/8.1.29

module list

julia --project -e 'import Pkg'
julia --project -e 'using Pkg; Pkg.instantiate()'
julia --project -e 'using Pkg; Pkg.gc()'
# 2. Update packages to the environment that we need to use. not always necessary
julia --project -e 'using Pkg; Pkg.add("MPI"); Pkg.add("MPIPreferences"); Pkg.add("KernelAbstractions"); Pkg.add("Oceananigans"); Pkg.add("Printf"); Pkg.add("Statistics"); Pkg.add("Random")'
# Tell MPI that we would like to use the system binary we loaded with module load cray-mpich
julia --project -e 'using MPIPreferences; MPIPreferences.use_system_binary(vendor="cray", force=true)'

mpiexec -n $ntasks ./launch.sh julia --project langmuir_turb_forcing_cpu.jl
wait
mpiexec -n $ntasks ./launch.sh julia --project langmuir_turb_sgs_cpu.jl
EXEC

qsub < EXEC_STEP