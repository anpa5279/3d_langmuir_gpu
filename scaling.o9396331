
Currently Loaded Modules:
  1) ncarenv/23.09 (S)   4) craype/2.7.31          7) cray-mpich/8.1.29
  2) nvhpc/24.7          5) ncarcompilers/1.0.0
  3) cuda/12.2.1         6) gcc-toolchain/13.2.0

  Where:
   S:  Module is Sticky, requires --force to unload or purge

 

┌ Info: MPI implementation identified
│   libmpi = "libmpi_nvidia.so"
│   version_string = "MPI VERSION    : CRAY MPICH version 8.1.29.34 (ANL base 3.4a2)\nMPI BUILD INFO : Tue Feb 20 20:43 2024 (git hash d8ab47f)\n"
│   impl = "CrayMPICH"
│   version = v"8.1.29"
└   abi = "MPICH"
┌ Info: MPIPreferences unchanged
│   binary = "system"
│   libmpi = "libmpi_nvidia.so"
│   abi = "MPICH"
│   mpiexec = "mpiexec"
│   preloads = 1-element Vector{String}: …
└   preloads_env_switch = "MPICH_GPU_SUPPORT_ENABLED"
┌ Warning: CUDA runtime library `libcudart.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcudart.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
┌ Warning: CUDA runtime library `libcublasLt.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcublasLt.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
┌ Warning: CUDA runtime library `libnvJitLink.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libnvJitLink.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
┌ Warning: CUDA runtime library `libcusparse.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcusparse.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
Global Rank 0 / Local Rank 0 / CUDA_VISIBLE_DEVICES=0 / deg0004
┌ Warning: CUDA runtime library `libcudart.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcudart.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
┌ Warning: CUDA runtime library `libcublasLt.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcublasLt.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
┌ Warning: CUDA runtime library `libnvJitLink.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libnvJitLink.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
┌ Warning: CUDA runtime library `libcusparse.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcusparse.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
[ Info: Oceananigans will use 64 threads
Hello from process 0 out of 1
u_bcs = Oceananigans.FieldBoundaryConditions, with boundary conditions
├── west: DefaultBoundaryCondition (FluxBoundaryCondition: Nothing)
├── east: DefaultBoundaryCondition (FluxBoundaryCondition: Nothing)
├── south: DefaultBoundaryCondition (FluxBoundaryCondition: Nothing)
├── north: DefaultBoundaryCondition (FluxBoundaryCondition: Nothing)
├── bottom: DefaultBoundaryCondition (FluxBoundaryCondition: Nothing)
├── top: FluxBoundaryCondition: -7.50208e-5
└── immersed: DefaultBoundaryCondition (FluxBoundaryCondition: Nothing)
buoyancy = SeawaterBuoyancy{Float64}:
├── gravitational_acceleration: 9.80665
├── constant_salinity: 35.0
└── equation_of_state: LinearEquationOfState(thermal_expansion=0.0002, haline_contraction=0.00078)
model = NonhydrostaticModel{GPU, RectilinearGrid}(time = 0 seconds, iteration = 0)
├── grid: 384×384×384 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on CUDAGPU with 3×3×3 halo
├── timestepper: RungeKutta3TimeStepper
├── advection scheme: WENO(order=5)
├── tracers: T
├── closure: AnisotropicMinimumDissipation{Oceananigans.TurbulenceClosures.ExplicitTimeDiscretization, @NamedTuple{T::Float64}, Float64, Nothing}
├── buoyancy: SeawaterBuoyancy with g=9.80665 and LinearEquationOfState(thermal_expansion=0.0002, haline_contraction=0.00078) with ĝ = NegativeZDirection()
└── coriolis: FPlane{Float64}(f=0.0001)
simulation = Simulation of NonhydrostaticModel{GPU, RectilinearGrid}(time = 0 seconds, iteration = 0)
├── Next time step: 30 seconds
├── Elapsed wall time: 0 seconds
├── Wall time per iteration: NaN days
├── Stop time: 4 hours
├── Stop iteration: Inf
├── Wall time limit: Inf
├── Minimum relative step: 0.0
├── Callbacks: OrderedDict with 4 entries:
│   ├── stop_time_exceeded => Callback of stop_time_exceeded on IterationInterval(1)
│   ├── stop_iteration_exceeded => Callback of stop_iteration_exceeded on IterationInterval(1)
│   ├── wall_time_limit_exceeded => Callback of wall_time_limit_exceeded on IterationInterval(1)
│   └── nan_checker => Callback of NaNChecker for u on IterationInterval(100)
├── Output writers: OrderedDict with no entries
└── Diagnostics: OrderedDict with no entries
[ Info: Initializing simulation...
[ Info: iteration: 0, time: 0 seconds, wall time: 22.389 seconds, max|w|: 1.498e-03, m s⁻¹
[ Info:     ... simulation initialization complete (27.821 seconds)
[ Info: Executing initial time step...
[ Info:     ... initial time step complete (3.717 seconds).
[ Info: iteration: 20, time: 7.500 minutes, wall time: 13.667 seconds, max|w|:    NaN, m s⁻¹
[ Info: iteration: 40, time: NaN days, wall time: 6.422 seconds, max|w|:    NaN, m s⁻¹
[ Info: iteration: 60, time: NaN days, wall time: 6.418 seconds, max|w|:    NaN, m s⁻¹
[ Info: iteration: 80, time: NaN days, wall time: 6.424 seconds, max|w|:    NaN, m s⁻¹
[ Info: time = NaN, iteration = 100: NaN found in field u. Stopping simulation.
[ Info: iteration: 100, time: NaN days, wall time: 6.442 seconds, max|w|:    NaN, m s⁻¹
setup_end - setup_start = 128.35782194137573
start1 = 1.746205526319701e9
end1 = 1.746205591108595e9
end1 - start1 = 64.78889393806458
simulation.model.clock.iteration = 100
